\providecommand{\bbobecdfcaptionsinglefunctionssingledim}[1]{
Empirical cumulative distribution of simulated (bootstrapped)
             runtimes, measured in number of objective function evaluations 
             divided by dimension (FEvals/DIM) in 
             dimension #1 and for those targets in
             $10^{[-8..2]}$ that have just not been reached by 
             the best algorithm from BBOB 2009 in a given budget of $k$ $\times$ DIM, with 
             $31$ different values of $k$ chosen 
             equidistant in logscale within the interval $\{0.5, \dots, 50\}$.
}
\providecommand{\cocoversion}{\hspace{\textwidth}\scriptsize\sffamily{}\color{Gray}Data produced with COCO v2.0.1}
\providecommand{\bbobECDFslegend}[1]{
Bootstrapped empirical cumulative distribution of the number of objective function evaluations divided by dimension (FEvals/DIM) for all functions and subgroups in #1-D. The targets are chosen from $10^{[-8..2]}$ such that the best algorithm from BBOB 2009 just not reached them within a given budget of $k$ $\times$ DIM, with $31$ different values of $k$ chosen equidistant in logscale within the interval $\{0.5, \dots, 50\}$. The ``best 2009'' line corresponds to the best \aRT\ observed during BBOB 2009 for each selected target.
}
\providecommand{\bbobppfigslegend}[1]{
Average running time (\aRT\ in number of $f$-evaluations
                        as $\log_{10}$ value) divided by dimension versus dimension. The target function value
                        is chosen such that the best algorithm from BBOB 2009 just failed to achieve
                        an \aRT\ of $10\times\DIM$. Different symbols correspond to different algorithms given in the legend of #1. Light symbols give the maximum number of function evaluations from the longest trial divided by dimension. Black stars indicate a statistically better result compared to all other algorithms with $p<0.01$ and Bonferroni correction number of dimensions (six).  
Legend: 
{\color{NavyBlue}$\circ$}: \algorithmA
, {\color{Magenta}$\diamondsuit$}: \algorithmB
, {\color{Orange}$\star$}: \algorithmC
, {\color{CornflowerBlue}$\triangledown$}: \algorithmD
, {\color{red}$\varhexagon$}: \algorithmE
}
% define some COCO/dvipsnames colors because
% ACM style does not allow to use them directly
\definecolor{NavyBlue}{HTML}{000080}
\definecolor{Magenta}{HTML}{FF00FF}
\definecolor{Orange}{HTML}{FFA500}
\definecolor{CornflowerBlue}{HTML}{6495ED}
\definecolor{YellowGreen}{HTML}{9ACD32}
\definecolor{Gray}{HTML}{BEBEBE}
\definecolor{Yellow}{HTML}{FFFF00}
\definecolor{GreenYellow}{HTML}{ADFF2F}
\definecolor{ForestGreen}{HTML}{228B22}
\definecolor{Lavender}{HTML}{FFC0CB}
\definecolor{SkyBlue}{HTML}{87CEEB}
\definecolor{NavyBlue}{HTML}{000080}
\definecolor{Goldenrod}{HTML}{DDF700}
\definecolor{VioletRed}{HTML}{D02090}
\definecolor{CornflowerBlue}{HTML}{6495ED}
\definecolor{LimeGreen}{HTML}{32CD32}

\providecommand{\bbobpptablesmanylegend}[2]{%
        Average runtime (\aRT\ in number of function 
        evaluations) divided by the respective best \aRT\ measured during BBOB-2009 in
        #1.
        The \aRT\ and in braces, as dispersion measure, the half difference between 
        10 and 90\%-tile of bootstrapped run lengths appear for each algorithm and 
        %
        run-length based target, the corresponding reference \aRT\
        (preceded by the target \Df-value in \textit{italics}) in the first row. 
        \#succ is the number of trials that reached the target value of the last column.
        %
        The median number of conducted function evaluations is additionally given in 
        \textit{italics}, if the target in the last column was never reached. 
        Entries, succeeded by a star, are statistically significantly better (according to
        the rank-sum test) when compared to all other algorithms of the table, with
        $p = 0.05$ or $p = 10^{-k}$ when the number $k$ following the star is larger
        than 1, with Bonferroni correction of #2. A $\downarrow$ indicates the same tested against the best algorithm from BBOB 2009. Best results are printed in bold.
        \cocoversion}
\providecommand{\algorithmA}{Ord-N-DTS}
\providecommand{\algorithmB}{Ord-H-DTS}
\providecommand{\algorithmC}{Ord-Q-DTS}
\providecommand{\algorithmD}{DTS-CMA-ES}
\providecommand{\algorithmE}{CMA-ES}
\providecommand{\algname}{CMA-ES{}}
\providecommand{\algfolder}{CMA-ES/}
\providecommand{\bbobecdfcaptionallgroups}[1]{
Empirical cumulative distribution of simulated (bootstrapped)
             runtimes, measured in number of objective function evaluations 
             divided by dimension (FEvals/DIM) for all function groups and all 
             dimensions and for those targets in
             $10^{[-8..2]}$ that have just not been reached by 
             the best algorithm from BBOB 2009 in a given budget of $k$ $\times$ DIM, with 
             $31$ different values of $k$ chosen 
             equidistant in logscale within the interval $\{0.5, \dots, 50\}$.             
             The aggregation over all 24 
             functions is shown in the last plot.
}
\providecommand{\bbobecdfcaptionsinglefcts}[2]{
Empirical cumulative distribution of simulated (bootstrapped) runtimes in number
             of objective function evaluations divided by dimension (FEvals/DIM) for  
             targets in $10^{[-8..2]}$ that have just not
             been reached by the best algorithm from BBOB 2009
             in a given budget of $k$ $\times$ DIM, with $31$ 
             different values of $k$ chosen equidistant in logscale within the interval $\{0.5, \dots, 50\}$.
             Shown are functions $f_{#1}$ to $f_{#2}$ and all dimensions. 
}
\providecommand{\bbobpptablecaption}[1]{
%
        Average running time (\aRT\ in number of function 
        evaluations) divided by the \aRT\ of the best algorithm from BBOB 2009 in #1. The \aRT\ 
        and in braces, as dispersion measure, the half difference between 90 and 
        10\%-tile of bootstrapped run lengths appear in the second row of each cell,  
        the best \aRT\
        %
        in the first. The different target \Df-values are shown in the top row. 
        \#succ is the number of trials that reached the (final) target $\fopt 
        + 10^{-8}$.
        %
        The median number of conducted function evaluations is additionally given in 
        \textit{italics}, if the target in the last column was never reached. 
        \textbf{Bold} entries are statistically significantly better (according to
        the rank-sum test) compared to the best algorithm from BBOB 2009, with
        $p = 0.05$ or $p = 10^{-k}$ when the number $k > 1$ is following the
        $\downarrow$ symbol, with Bonferroni correction by the number of
        functions.\cocoversion
        
}
\providecommand{\bbobppfigdimlegend}[1]{
%
        Scaling of runtime with dimension to reach certain target values \Df.
        Lines: average runtime (\aRT);
        Cross (+): median runtime of successful runs to reach the most difficult
        target that was reached at least once (but not always);
        Cross ({\color{red}$\times$}): maximum number of
        $f$-evaluations in any trial. Notched boxes: interquartile range with median of simulated runs; 
        All values are divided by dimension and  
        plotted as $\log_{10}$ values versus dimension. %
        %
        Shown is the \aRT\ for targets just not reached by the best algorithm from BBOB 2009
        within the given budget $k\times\DIM$, where $k$ is shown in the
        legend. Numbers above \aRT-symbols (if appearing) indicate the number
        of trials reaching the respective target. The light thick line with diamonds indicates the best algorithm from BBOB 2009 for the most difficult target.  Slanted
        grid lines indicate a scaling with $\mathcal O$$(\DIM)$ compared to
        $\mathcal O$$(1)$ when using the respective reference algorithm.
        
}
\providecommand{\bbobpprldistrlegend}[1]{
%
         Empirical cumulative distribution functions (ECDF), plotting the fraction of
         trials with an outcome not larger than the respective value on the $x$-axis.
         #1%
         Left subplots: ECDF of number of function evaluations (FEvals) divided by search space dimension $D$,
         to fall below $\fopt+\Df$ where \Df{} is the
         target just not reached by the best algorithm from BBOB 2009 within a budget of
         $k\times\DIM$ evaluations, where $k$ is the first value in the legend. %
         Legends indicate for each target the number of functions that were solved in at
         least one trial within the displayed budget.
         Right subplots: ECDF of the best achieved $\Df$
         for running times of $0.5D, 1.2D, 3D, 10D, 100D, 1000D,\dots$
         function evaluations
         (from right to left cycling cyan-magenta-black\dots) and final $\Df$-value (red),
         where \Df and \textsf{Df} denote the difference to the optimal function value. 
         Light brown lines in the background show ECDFs for the most difficult target of all
            algorithms benchmarked during BBOB-2009.
}
\providecommand{\bbobloglossfigurecaption}[1]{
%
        \aRT\ loss ratios (see Figure~\ref{tab:aRTloss} for details).

        Each cross ({\color{blue}$+$}) represents a single function, the line
        is the geometric mean.
        
}
\providecommand{\bbobloglosstablecaption}[1]{
%
        \aRT\ loss ratio versus the budget in number of $f$-evaluations
        divided by dimension.
        For each given budget \FEvals, the target value \ftarget\ is computed
        as the best target $f$-value reached within the
        budget by the given algorithm.
        Shown is then the \aRT\ to reach \ftarget\ for the given algorithm
        or the budget, if the best algorithm from BBOB 2009
        reached a better target within the budget,
        divided by the \aRT\ of the best algorithm from BBOB 2009 to reach \ftarget.
        Line: geometric mean. Box-Whisker error bar: 25-75\%-ile with median
        (box), 10-90\%-ile (caps), and minimum and maximum \aRT\ loss ratio
        (points). The vertical line gives the maximal number of function evaluations
        in a single trial in this function subset. See also
        Figure~\ref{fig:aRTlogloss} for results on each function subgroup.\cocoversion
        
}
\providecommand{\algname}{DTS-CMA-ES{}}
\providecommand{\algfolder}{DTS-CMA-ES/}
\providecommand{\bbobecdfcaptionallgroups}[1]{
Empirical cumulative distribution of simulated (bootstrapped)
             runtimes, measured in number of objective function evaluations 
             divided by dimension (FEvals/DIM) for all function groups and all 
             dimensions and for those targets in
             $10^{[-8..2]}$ that have just not been reached by 
             the best algorithm from BBOB 2009 in a given budget of $k$ $\times$ DIM, with 
             $31$ different values of $k$ chosen 
             equidistant in logscale within the interval $\{0.5, \dots, 50\}$.             
             The aggregation over all 24 
             functions is shown in the last plot.
}
\providecommand{\bbobecdfcaptionsinglefcts}[2]{
Empirical cumulative distribution of simulated (bootstrapped) runtimes in number
             of objective function evaluations divided by dimension (FEvals/DIM) for  
             targets in $10^{[-8..2]}$ that have just not
             been reached by the best algorithm from BBOB 2009
             in a given budget of $k$ $\times$ DIM, with $31$ 
             different values of $k$ chosen equidistant in logscale within the interval $\{0.5, \dots, 50\}$.
             Shown are functions $f_{#1}$ to $f_{#2}$ and all dimensions. 
}
\providecommand{\bbobpptablecaption}[1]{
%
        Average running time (\aRT\ in number of function 
        evaluations) divided by the \aRT\ of the best algorithm from BBOB 2009 in #1. The \aRT\ 
        and in braces, as dispersion measure, the half difference between 90 and 
        10\%-tile of bootstrapped run lengths appear in the second row of each cell,  
        the best \aRT\
        %
        in the first. The different target \Df-values are shown in the top row. 
        \#succ is the number of trials that reached the (final) target $\fopt 
        + 10^{-8}$.
        %
        The median number of conducted function evaluations is additionally given in 
        \textit{italics}, if the target in the last column was never reached. 
        \textbf{Bold} entries are statistically significantly better (according to
        the rank-sum test) compared to the best algorithm from BBOB 2009, with
        $p = 0.05$ or $p = 10^{-k}$ when the number $k > 1$ is following the
        $\downarrow$ symbol, with Bonferroni correction by the number of
        functions.\cocoversion
        
}
\providecommand{\bbobppfigdimlegend}[1]{
%
        Scaling of runtime with dimension to reach certain target values \Df.
        Lines: average runtime (\aRT);
        Cross (+): median runtime of successful runs to reach the most difficult
        target that was reached at least once (but not always);
        Cross ({\color{red}$\times$}): maximum number of
        $f$-evaluations in any trial. Notched boxes: interquartile range with median of simulated runs; 
        All values are divided by dimension and  
        plotted as $\log_{10}$ values versus dimension. %
        %
        Shown is the \aRT\ for targets just not reached by the best algorithm from BBOB 2009
        within the given budget $k\times\DIM$, where $k$ is shown in the
        legend. Numbers above \aRT-symbols (if appearing) indicate the number
        of trials reaching the respective target. The light thick line with diamonds indicates the best algorithm from BBOB 2009 for the most difficult target.  Slanted
        grid lines indicate a scaling with $\mathcal O$$(\DIM)$ compared to
        $\mathcal O$$(1)$ when using the respective reference algorithm.
        
}
\providecommand{\bbobpprldistrlegend}[1]{
%
         Empirical cumulative distribution functions (ECDF), plotting the fraction of
         trials with an outcome not larger than the respective value on the $x$-axis.
         #1%
         Left subplots: ECDF of number of function evaluations (FEvals) divided by search space dimension $D$,
         to fall below $\fopt+\Df$ where \Df{} is the
         target just not reached by the best algorithm from BBOB 2009 within a budget of
         $k\times\DIM$ evaluations, where $k$ is the first value in the legend. %
         Legends indicate for each target the number of functions that were solved in at
         least one trial within the displayed budget.
         Right subplots: ECDF of the best achieved $\Df$
         for running times of $0.5D, 1.2D, 3D, 10D, 100D, 1000D,\dots$
         function evaluations
         (from right to left cycling cyan-magenta-black\dots) and final $\Df$-value (red),
         where \Df and \textsf{Df} denote the difference to the optimal function value. 
         Light brown lines in the background show ECDFs for the most difficult target of all
            algorithms benchmarked during BBOB-2009.
}
\providecommand{\bbobloglossfigurecaption}[1]{
%
        \aRT\ loss ratios (see Figure~\ref{tab:aRTloss} for details).

        Each cross ({\color{blue}$+$}) represents a single function, the line
        is the geometric mean.
        
}
\providecommand{\bbobloglosstablecaption}[1]{
%
        \aRT\ loss ratio versus the budget in number of $f$-evaluations
        divided by dimension.
        For each given budget \FEvals, the target value \ftarget\ is computed
        as the best target $f$-value reached within the
        budget by the given algorithm.
        Shown is then the \aRT\ to reach \ftarget\ for the given algorithm
        or the budget, if the best algorithm from BBOB 2009
        reached a better target within the budget,
        divided by the \aRT\ of the best algorithm from BBOB 2009 to reach \ftarget.
        Line: geometric mean. Box-Whisker error bar: 25-75\%-ile with median
        (box), 10-90\%-ile (caps), and minimum and maximum \aRT\ loss ratio
        (points). The vertical line gives the maximal number of function evaluations
        in a single trial in this function subset. See also
        Figure~\ref{fig:aRTlogloss} for results on each function subgroup.\cocoversion
        
}
\providecommand{\algname}{Ord-Q-DTS{}}
\providecommand{\algfolder}{Ord-Q-DTS/}
\providecommand{\bbobecdfcaptionallgroups}[1]{
Empirical cumulative distribution of simulated (bootstrapped)
             runtimes, measured in number of objective function evaluations 
             divided by dimension (FEvals/DIM) for all function groups and all 
             dimensions and for those targets in
             $10^{[-8..2]}$ that have just not been reached by 
             the best algorithm from BBOB 2009 in a given budget of $k$ $\times$ DIM, with 
             $31$ different values of $k$ chosen 
             equidistant in logscale within the interval $\{0.5, \dots, 50\}$.             
             The aggregation over all 24 
             functions is shown in the last plot.
}
\providecommand{\bbobecdfcaptionsinglefcts}[2]{
Empirical cumulative distribution of simulated (bootstrapped) runtimes in number
             of objective function evaluations divided by dimension (FEvals/DIM) for  
             targets in $10^{[-8..2]}$ that have just not
             been reached by the best algorithm from BBOB 2009
             in a given budget of $k$ $\times$ DIM, with $31$ 
             different values of $k$ chosen equidistant in logscale within the interval $\{0.5, \dots, 50\}$.
             Shown are functions $f_{#1}$ to $f_{#2}$ and all dimensions. 
}
\providecommand{\bbobpptablecaption}[1]{
%
        Average running time (\aRT\ in number of function 
        evaluations) divided by the \aRT\ of the best algorithm from BBOB 2009 in #1. The \aRT\ 
        and in braces, as dispersion measure, the half difference between 90 and 
        10\%-tile of bootstrapped run lengths appear in the second row of each cell,  
        the best \aRT\
        %
        in the first. The different target \Df-values are shown in the top row. 
        \#succ is the number of trials that reached the (final) target $\fopt 
        + 10^{-8}$.
        %
        The median number of conducted function evaluations is additionally given in 
        \textit{italics}, if the target in the last column was never reached. 
        \textbf{Bold} entries are statistically significantly better (according to
        the rank-sum test) compared to the best algorithm from BBOB 2009, with
        $p = 0.05$ or $p = 10^{-k}$ when the number $k > 1$ is following the
        $\downarrow$ symbol, with Bonferroni correction by the number of
        functions.\cocoversion
        
}
\providecommand{\bbobppfigdimlegend}[1]{
%
        Scaling of runtime with dimension to reach certain target values \Df.
        Lines: average runtime (\aRT);
        Cross (+): median runtime of successful runs to reach the most difficult
        target that was reached at least once (but not always);
        Cross ({\color{red}$\times$}): maximum number of
        $f$-evaluations in any trial. Notched boxes: interquartile range with median of simulated runs; 
        All values are divided by dimension and  
        plotted as $\log_{10}$ values versus dimension. %
        %
        Shown is the \aRT\ for targets just not reached by the best algorithm from BBOB 2009
        within the given budget $k\times\DIM$, where $k$ is shown in the
        legend. Numbers above \aRT-symbols (if appearing) indicate the number
        of trials reaching the respective target. The light thick line with diamonds indicates the best algorithm from BBOB 2009 for the most difficult target.  Slanted
        grid lines indicate a scaling with $\mathcal O$$(\DIM)$ compared to
        $\mathcal O$$(1)$ when using the respective reference algorithm.
        
}
\providecommand{\bbobpprldistrlegend}[1]{
%
         Empirical cumulative distribution functions (ECDF), plotting the fraction of
         trials with an outcome not larger than the respective value on the $x$-axis.
         #1%
         Left subplots: ECDF of number of function evaluations (FEvals) divided by search space dimension $D$,
         to fall below $\fopt+\Df$ where \Df{} is the
         target just not reached by the best algorithm from BBOB 2009 within a budget of
         $k\times\DIM$ evaluations, where $k$ is the first value in the legend. %
         Legends indicate for each target the number of functions that were solved in at
         least one trial within the displayed budget.
         Right subplots: ECDF of the best achieved $\Df$
         for running times of $0.5D, 1.2D, 3D, 10D, 100D, 1000D,\dots$
         function evaluations
         (from right to left cycling cyan-magenta-black\dots) and final $\Df$-value (red),
         where \Df and \textsf{Df} denote the difference to the optimal function value. 
         Light brown lines in the background show ECDFs for the most difficult target of all
            algorithms benchmarked during BBOB-2009.
}
\providecommand{\bbobloglossfigurecaption}[1]{
%
        \aRT\ loss ratios (see Figure~\ref{tab:aRTloss} for details).

        Each cross ({\color{blue}$+$}) represents a single function, the line
        is the geometric mean.
        
}
\providecommand{\bbobloglosstablecaption}[1]{
%
        \aRT\ loss ratio versus the budget in number of $f$-evaluations
        divided by dimension.
        For each given budget \FEvals, the target value \ftarget\ is computed
        as the best target $f$-value reached within the
        budget by the given algorithm.
        Shown is then the \aRT\ to reach \ftarget\ for the given algorithm
        or the budget, if the best algorithm from BBOB 2009
        reached a better target within the budget,
        divided by the \aRT\ of the best algorithm from BBOB 2009 to reach \ftarget.
        Line: geometric mean. Box-Whisker error bar: 25-75\%-ile with median
        (box), 10-90\%-ile (caps), and minimum and maximum \aRT\ loss ratio
        (points). The vertical line gives the maximal number of function evaluations
        in a single trial in this function subset. See also
        Figure~\ref{fig:aRTlogloss} for results on each function subgroup.\cocoversion
        
}
\providecommand{\algname}{Ord-H-DTS{}}
\providecommand{\algfolder}{Ord-H-DTS/}
\providecommand{\bbobecdfcaptionallgroups}[1]{
Empirical cumulative distribution of simulated (bootstrapped)
             runtimes, measured in number of objective function evaluations 
             divided by dimension (FEvals/DIM) for all function groups and all 
             dimensions and for those targets in
             $10^{[-8..2]}$ that have just not been reached by 
             the best algorithm from BBOB 2009 in a given budget of $k$ $\times$ DIM, with 
             $31$ different values of $k$ chosen 
             equidistant in logscale within the interval $\{0.5, \dots, 50\}$.             
             The aggregation over all 24 
             functions is shown in the last plot.
}
\providecommand{\bbobecdfcaptionsinglefcts}[2]{
Empirical cumulative distribution of simulated (bootstrapped) runtimes in number
             of objective function evaluations divided by dimension (FEvals/DIM) for  
             targets in $10^{[-8..2]}$ that have just not
             been reached by the best algorithm from BBOB 2009
             in a given budget of $k$ $\times$ DIM, with $31$ 
             different values of $k$ chosen equidistant in logscale within the interval $\{0.5, \dots, 50\}$.
             Shown are functions $f_{#1}$ to $f_{#2}$ and all dimensions. 
}
\providecommand{\bbobpptablecaption}[1]{
%
        Average running time (\aRT\ in number of function 
        evaluations) divided by the \aRT\ of the best algorithm from BBOB 2009 in #1. The \aRT\ 
        and in braces, as dispersion measure, the half difference between 90 and 
        10\%-tile of bootstrapped run lengths appear in the second row of each cell,  
        the best \aRT\
        %
        in the first. The different target \Df-values are shown in the top row. 
        \#succ is the number of trials that reached the (final) target $\fopt 
        + 10^{-8}$.
        %
        The median number of conducted function evaluations is additionally given in 
        \textit{italics}, if the target in the last column was never reached. 
        \textbf{Bold} entries are statistically significantly better (according to
        the rank-sum test) compared to the best algorithm from BBOB 2009, with
        $p = 0.05$ or $p = 10^{-k}$ when the number $k > 1$ is following the
        $\downarrow$ symbol, with Bonferroni correction by the number of
        functions.\cocoversion
        
}
\providecommand{\bbobppfigdimlegend}[1]{
%
        Scaling of runtime with dimension to reach certain target values \Df.
        Lines: average runtime (\aRT);
        Cross (+): median runtime of successful runs to reach the most difficult
        target that was reached at least once (but not always);
        Cross ({\color{red}$\times$}): maximum number of
        $f$-evaluations in any trial. Notched boxes: interquartile range with median of simulated runs; 
        All values are divided by dimension and  
        plotted as $\log_{10}$ values versus dimension. %
        %
        Shown is the \aRT\ for targets just not reached by the best algorithm from BBOB 2009
        within the given budget $k\times\DIM$, where $k$ is shown in the
        legend. Numbers above \aRT-symbols (if appearing) indicate the number
        of trials reaching the respective target. The light thick line with diamonds indicates the best algorithm from BBOB 2009 for the most difficult target.  Slanted
        grid lines indicate a scaling with $\mathcal O$$(\DIM)$ compared to
        $\mathcal O$$(1)$ when using the respective reference algorithm.
        
}
\providecommand{\bbobpprldistrlegend}[1]{
%
         Empirical cumulative distribution functions (ECDF), plotting the fraction of
         trials with an outcome not larger than the respective value on the $x$-axis.
         #1%
         Left subplots: ECDF of number of function evaluations (FEvals) divided by search space dimension $D$,
         to fall below $\fopt+\Df$ where \Df{} is the
         target just not reached by the best algorithm from BBOB 2009 within a budget of
         $k\times\DIM$ evaluations, where $k$ is the first value in the legend. %
         Legends indicate for each target the number of functions that were solved in at
         least one trial within the displayed budget.
         Right subplots: ECDF of the best achieved $\Df$
         for running times of $0.5D, 1.2D, 3D, 10D, 100D, 1000D,\dots$
         function evaluations
         (from right to left cycling cyan-magenta-black\dots) and final $\Df$-value (red),
         where \Df and \textsf{Df} denote the difference to the optimal function value. 
         Light brown lines in the background show ECDFs for the most difficult target of all
            algorithms benchmarked during BBOB-2009.
}
\providecommand{\bbobloglossfigurecaption}[1]{
%
        \aRT\ loss ratios (see Figure~\ref{tab:aRTloss} for details).

        Each cross ({\color{blue}$+$}) represents a single function, the line
        is the geometric mean.
        
}
\providecommand{\bbobloglosstablecaption}[1]{
%
        \aRT\ loss ratio versus the budget in number of $f$-evaluations
        divided by dimension.
        For each given budget \FEvals, the target value \ftarget\ is computed
        as the best target $f$-value reached within the
        budget by the given algorithm.
        Shown is then the \aRT\ to reach \ftarget\ for the given algorithm
        or the budget, if the best algorithm from BBOB 2009
        reached a better target within the budget,
        divided by the \aRT\ of the best algorithm from BBOB 2009 to reach \ftarget.
        Line: geometric mean. Box-Whisker error bar: 25-75\%-ile with median
        (box), 10-90\%-ile (caps), and minimum and maximum \aRT\ loss ratio
        (points). The vertical line gives the maximal number of function evaluations
        in a single trial in this function subset. See also
        Figure~\ref{fig:aRTlogloss} for results on each function subgroup.\cocoversion
        
}
\providecommand{\algname}{Ord-N-DTS{}}
\providecommand{\algfolder}{Ord-N-DTS/}
\providecommand{\bbobecdfcaptionallgroups}[1]{
Empirical cumulative distribution of simulated (bootstrapped)
             runtimes, measured in number of objective function evaluations 
             divided by dimension (FEvals/DIM) for all function groups and all 
             dimensions and for those targets in
             $10^{[-8..2]}$ that have just not been reached by 
             the best algorithm from BBOB 2009 in a given budget of $k$ $\times$ DIM, with 
             $31$ different values of $k$ chosen 
             equidistant in logscale within the interval $\{0.5, \dots, 50\}$.             
             The aggregation over all 24 
             functions is shown in the last plot.
}
\providecommand{\bbobecdfcaptionsinglefcts}[2]{
Empirical cumulative distribution of simulated (bootstrapped) runtimes in number
             of objective function evaluations divided by dimension (FEvals/DIM) for  
             targets in $10^{[-8..2]}$ that have just not
             been reached by the best algorithm from BBOB 2009
             in a given budget of $k$ $\times$ DIM, with $31$ 
             different values of $k$ chosen equidistant in logscale within the interval $\{0.5, \dots, 50\}$.
             Shown are functions $f_{#1}$ to $f_{#2}$ and all dimensions. 
}
\providecommand{\bbobpptablecaption}[1]{
%
        Average running time (\aRT\ in number of function 
        evaluations) divided by the \aRT\ of the best algorithm from BBOB 2009 in #1. The \aRT\ 
        and in braces, as dispersion measure, the half difference between 90 and 
        10\%-tile of bootstrapped run lengths appear in the second row of each cell,  
        the best \aRT\
        %
        in the first. The different target \Df-values are shown in the top row. 
        \#succ is the number of trials that reached the (final) target $\fopt 
        + 10^{-8}$.
        %
        The median number of conducted function evaluations is additionally given in 
        \textit{italics}, if the target in the last column was never reached. 
        \textbf{Bold} entries are statistically significantly better (according to
        the rank-sum test) compared to the best algorithm from BBOB 2009, with
        $p = 0.05$ or $p = 10^{-k}$ when the number $k > 1$ is following the
        $\downarrow$ symbol, with Bonferroni correction by the number of
        functions.\cocoversion
        
}
\providecommand{\bbobppfigdimlegend}[1]{
%
        Scaling of runtime with dimension to reach certain target values \Df.
        Lines: average runtime (\aRT);
        Cross (+): median runtime of successful runs to reach the most difficult
        target that was reached at least once (but not always);
        Cross ({\color{red}$\times$}): maximum number of
        $f$-evaluations in any trial. Notched boxes: interquartile range with median of simulated runs; 
        All values are divided by dimension and  
        plotted as $\log_{10}$ values versus dimension. %
        %
        Shown is the \aRT\ for targets just not reached by the best algorithm from BBOB 2009
        within the given budget $k\times\DIM$, where $k$ is shown in the
        legend. Numbers above \aRT-symbols (if appearing) indicate the number
        of trials reaching the respective target. The light thick line with diamonds indicates the best algorithm from BBOB 2009 for the most difficult target.  Slanted
        grid lines indicate a scaling with $\mathcal O$$(\DIM)$ compared to
        $\mathcal O$$(1)$ when using the respective reference algorithm.
        
}
\providecommand{\bbobpprldistrlegend}[1]{
%
         Empirical cumulative distribution functions (ECDF), plotting the fraction of
         trials with an outcome not larger than the respective value on the $x$-axis.
         #1%
         Left subplots: ECDF of number of function evaluations (FEvals) divided by search space dimension $D$,
         to fall below $\fopt+\Df$ where \Df{} is the
         target just not reached by the best algorithm from BBOB 2009 within a budget of
         $k\times\DIM$ evaluations, where $k$ is the first value in the legend. %
         Legends indicate for each target the number of functions that were solved in at
         least one trial within the displayed budget.
         Right subplots: ECDF of the best achieved $\Df$
         for running times of $0.5D, 1.2D, 3D, 10D, 100D, 1000D,\dots$
         function evaluations
         (from right to left cycling cyan-magenta-black\dots) and final $\Df$-value (red),
         where \Df and \textsf{Df} denote the difference to the optimal function value. 
         Light brown lines in the background show ECDFs for the most difficult target of all
            algorithms benchmarked during BBOB-2009.
}
\providecommand{\bbobloglossfigurecaption}[1]{
%
        \aRT\ loss ratios (see Figure~\ref{tab:aRTloss} for details).

        Each cross ({\color{blue}$+$}) represents a single function, the line
        is the geometric mean.
        
}
\providecommand{\bbobloglosstablecaption}[1]{
%
        \aRT\ loss ratio versus the budget in number of $f$-evaluations
        divided by dimension.
        For each given budget \FEvals, the target value \ftarget\ is computed
        as the best target $f$-value reached within the
        budget by the given algorithm.
        Shown is then the \aRT\ to reach \ftarget\ for the given algorithm
        or the budget, if the best algorithm from BBOB 2009
        reached a better target within the budget,
        divided by the \aRT\ of the best algorithm from BBOB 2009 to reach \ftarget.
        Line: geometric mean. Box-Whisker error bar: 25-75\%-ile with median
        (box), 10-90\%-ile (caps), and minimum and maximum \aRT\ loss ratio
        (points). The vertical line gives the maximal number of function evaluations
        in a single trial in this function subset. See also
        Figure~\ref{fig:aRTlogloss} for results on each function subgroup.\cocoversion
        
}
